import re
import json
import random
import os
from typing import Dict, List, Any
import numpy as np
from collections import defaultdict
from argparse import ArgumentParser
import openai
from openai import OpenAI
import time
import concurrent.futures
from tqdm import tqdm

def get_args():
    parser = ArgumentParser(
        description="(å…¨åŠŸèƒ½ç‰ˆ) ä½¿ç”¨GPT-4oå¹¶è¡Œè¯„æµ‹Pass@kï¼Œå…·å¤‡æ–­ç‚¹ç»­ä¼ ã€æ™ºèƒ½é‡è¯•å’Œå¯é…ç½®APIç«¯ç‚¹ã€‚"
    )
    parser.add_argument('--prediction_file', type=str, required=True, help="æ¨ç†è„šæœ¬ç”Ÿæˆçš„åŒ…å«å¤šæ¬¡å°è¯•çš„ç»“æœæ–‡ä»¶")
    parser.add_argument('--ground_truth_file', type=str, required=True, help="åŸå§‹æ•°æ®æ–‡ä»¶ä½œä¸ºæ ‡å‡†ç­”æ¡ˆ")
    parser.add_argument('--output_file', type=str, required=True, help="ä¿å­˜è¯¦ç»†è¯„æµ‹ç»“æœçš„JSONæ–‡ä»¶")
    parser.add_argument(
        '--api_key', 
        type=str, 
        default=os.getenv("OPENAI_API_KEY"),
        help="API Keyã€‚é»˜è®¤ä»ç¯å¢ƒå˜é‡ OPENAI_API_KEY è¯»å–ã€‚"
    )
    parser.add_argument(
        '--base_url', 
        type=str, 
        default=os.getenv("OPENAI_BASE_URL", "https://aihubmix.com/v1"),
        help="API Base URLã€‚é»˜è®¤ä»ç¯å¢ƒå˜é‡ OPENAI_BASE_URL è¯»å–ï¼Œè‹¥æ— åˆ™ä½¿ç”¨ç¡¬ç¼–ç å€¼ã€‚"
    )
    parser.add_argument('--model_name', type=str, default='gpt-4o-2024-05-13', help="ç”¨äºè¯„æµ‹çš„æ¨¡å‹åç§°")
    parser.add_argument('--parallel_size', type=int, default=32, help="æœ€å¤§å¹¶å‘APIè¯·æ±‚æ•°")
    parser.add_argument('--resume', action='store_true', help='ä»ä¸Šæ¬¡ä¸­æ–­çš„åœ°æ–¹ç»§ç»­è¯„æµ‹')
    args = parser.parse_args()
    return args


def read_jsonl(file_path: str) -> List[Dict[str, Any]]:
    data = []
    if not os.path.exists(file_path):
        return []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                data.append(json.loads(line))
            except json.JSONDecodeError:
                print(f"è­¦å‘Š: æ— æ³•è§£æè¡Œ: {line.strip()}")
    return data


class RobustPassKEvaluator:
    
    def __init__(self, api_key: str, model_name: str, base_url: str, parallel_size: int):
        self.api_key = api_key
        self.model_name = model_name
        self.base_url = base_url
        self.parallel_size = parallel_size
        self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)
        
        self.evaluation_prompt = """
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„LaTeXå…¬å¼è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°é¢„æµ‹çš„LaTeXå…¬å¼ä¸æ ‡å‡†ç­”æ¡ˆçš„åŒ¹é…ç¨‹åº¦ã€‚

        è¯„ä¼°æ ‡å‡†ï¼š
        1. ç²¾ç¡®åŒ¹é…ï¼šé¢„æµ‹å…¬å¼ä¸æ ‡å‡†ç­”æ¡ˆåœ¨æ•°å­¦æ„ä¹‰ä¸Šå®Œå…¨ç›¸åŒã€‚
        2. éƒ¨åˆ†åŒ¹é…ï¼šé¢„æµ‹å…¬å¼ä¸æ ‡å‡†ç­”æ¡ˆåœ¨æ•°å­¦æ„ä¹‰ä¸Šç›¸ä¼¼ï¼Œä½†å¯èƒ½æœ‰ç»†å¾®å·®å¼‚ã€‚
        3. ä¸åŒ¹é…ï¼šé¢„æµ‹å…¬å¼ä¸æ ‡å‡†ç­”æ¡ˆåœ¨æ•°å­¦æ„ä¹‰ä¸Šä¸åŒã€‚

        æ ‡å‡†ç­”æ¡ˆï¼š{ground_truth}
        é¢„æµ‹ç»“æœï¼š{prediction}

        è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼Œä¸è¦æ·»åŠ ä»»ä½•é¢å¤–è¯´æ˜ï¼š
        {{
            "exact_match": true/false,
            "partial_match": true/false,
            "explanation": "è¯„ä¼°è¯´æ˜"
        }}
    """

    def _validate_response(self, response: Any) -> bool:
        try:
            if not hasattr(response, 'choices') or not response.choices: return False
            message = response.choices[0].message
            if not hasattr(message, 'content') or not message.content.strip(): return False
            json.loads(message.content)
            return True
        except (AttributeError, json.JSONDecodeError, Exception):
            return False

    def call_gpt4o(self, prompt: str, max_retries: int = 10) -> Dict[str, Any]:
        for attempt in range(max_retries):
            try:
                time.sleep(random.uniform(0.1, 0.5))
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=1024,
                    temperature=0.0,
                    seed=1234,
                    response_format={"type": "json_object"}
                )
                
                if self._validate_response(response):
                    result_text = response.choices[0].message.content
                    result = json.loads(result_text)
                    if 'exact_match' in result and 'partial_match' in result:
                        return result
                else:
                    print(f"\nâŒ Invalid response format on try {attempt + 1}/{max_retries}. Response: {str(response)[:200]}...")

            except openai.RateLimitError as e:
                wait_time = 60
                print(f"\nâ³ Rate limit hit on try {attempt + 1}. Waiting for {wait_time}s... Error: {e}")
                time.sleep(wait_time)
            except openai.APIStatusError as e:
                print(f"\nğŸš« API Status Error on try {attempt + 1} (e.g., 500 server error). Retrying... Error: {e}")
                time.sleep(5)
            except Exception as e:
                print(f"\nğŸ’¥ Unexpected API error on try {attempt + 1}: {type(e).__name__}: {e}. Retrying with exponential backoff...")
                time.sleep(2 ** attempt)
        
        print(f"\nâŒ API call failed after all {max_retries} retries.")
        return {'exact_match': False, 'partial_match': False, 'explanation': f'APIè°ƒç”¨å¤±è´¥è¶…è¿‡{max_retries}æ¬¡'}

    def evaluate_single_pair(self, prediction: str, ground_truth: str) -> Dict[str, Any]:
        if not prediction or not ground_truth:
            return {'exact_match': False, 'partial_match': False, 'explanation': 'é¢„æµ‹æˆ–çœŸå€¼ä¸ºç©º'}
        
        # å…³é”®ï¼šå¯¹åæ–œæ è¿›è¡Œè½¬ä¹‰ä»¥ç¡®ä¿JSON payloadæœ‰æ•ˆ
        safe_prediction = prediction.replace('\\', '\\\\')
        safe_ground_truth = ground_truth.replace('\\', '\\\\')
            
        prompt = self.evaluation_prompt.format(ground_truth=safe_ground_truth, prediction=safe_prediction)
        return self.call_gpt4o(prompt)

    def evaluate_sample(self, prediction_item: Dict, ground_truth_item: Dict) -> Dict:
        sample_id = ground_truth_item.get('id', 'unknown_id')
        gt_answers = ground_truth_item.get('answers', [])
        pred_extracts = prediction_item.get('extract_answers', [])
        
        if not gt_answers:
            return {'id': sample_id, 'correct_attempts': 0, 'exact_match_attempts': 0, 'total_attempts': len(pred_extracts), 'per_attempt_details': []}
        
        first_gt_answer = gt_answers[0].get('content', '')
        
        combined_correct_count = 0
        exact_match_count = 0
        attempt_details = []

        with concurrent.futures.ThreadPoolExecutor(max_workers=self.parallel_size) as executor:
            future_to_extract = {
                executor.submit(self.evaluate_single_pair, extract.get('formula', ''), first_gt_answer): extract
                for extract in pred_extracts
            }
            
            for future in concurrent.futures.as_completed(future_to_extract):
                pred_extract = future_to_extract[future]
                try:
                    gpt_result = future.result()
                    
                    is_exact_match = gpt_result.get('exact_match', False)
                    if is_exact_match:
                        exact_match_count += 1
                    
                    is_combined_correct = is_exact_match or gpt_result.get('partial_match', False)
                    if is_combined_correct:
                        combined_correct_count += 1

                    attempt_details.append({
                        "attempted_formula": pred_extract.get('formula', ''),
                        "is_correct_combined": is_combined_correct,
                        "is_correct_exact": is_exact_match,
                        "gpt_evaluation": gpt_result
                    })
                except Exception as e:
                    attempt_details.append({"is_correct_combined": False, "is_correct_exact": False, "gpt_evaluation": {"error": str(e)}})

        return {
            'id': sample_id,
            'correct_attempts': combined_correct_count,
            'exact_match_attempts': exact_match_count,
            'total_attempts': len(pred_extracts),
            'ground_truth': first_gt_answer,
            'per_attempt_details': attempt_details
        }

    def batch_evaluate(self, predictions: List[Dict], ground_truths: List[Dict], intermediate_file_path: str) -> List[Dict]:
        if len(predictions) != len(ground_truths):
            raise ValueError("é¢„æµ‹å’ŒçœŸå€¼æ–‡ä»¶è¡Œæ•°ä¸åŒ¹é…!")

        all_sample_results = []
        
        with tqdm(total=len(predictions), desc="Evaluating Samples") as pbar:
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.parallel_size) as executor:
                future_to_sample_idx = {
                    executor.submit(self.evaluate_sample, pred, gt): idx
                    for idx, (pred, gt) in enumerate(zip(predictions, ground_truths))
                }
                
                for future in concurrent.futures.as_completed(future_to_sample_idx):
                    idx = future_to_sample_idx[future]
                    try:
                        sample_result = future.result()
                        sample_result['line_number'] = idx + 1
                        all_sample_results.append(sample_result)
                        
                        with open(intermediate_file_path, 'a', encoding='utf-8') as f:
                            f.write(json.dumps(sample_result, ensure_ascii=False) + '\n')
                    except Exception as e:
                        print(f"\n[!] Failed to process sample index {idx}: {e}")
                    pbar.update(1)

        return all_sample_results


def calculate_and_report_metrics(sample_results: List[Dict]):
    total_samples = len(sample_results)
    if total_samples == 0:
        print("æ²¡æœ‰å¯ä¾›åˆ†æçš„ç»“æœã€‚"); return {}

    combined_correct_list = [res.get('correct_attempts', 0) for res in sample_results]
    exact_match_list = [res.get('exact_match_attempts', 0) for res in sample_results]
    
    pass_at_k_thresholds = [8, 16]
    
    pass_counts_combined = {k: sum(1 for c in combined_correct_list if c >= k) for k in pass_at_k_thresholds}
    pass_rates_combined = {k: (pass_counts_combined[k] / total_samples) * 100 for k in pass_at_k_thresholds}

    pass_counts_exact = {k: sum(1 for c in exact_match_list if c >= k) for k in pass_at_k_thresholds}
    pass_rates_exact = {k: (pass_counts_exact[k] / total_samples) * 100 for k in pass_at_k_thresholds}
    
    descriptive_stats = {'mean': np.mean(combined_correct_list), 'median': np.median(combined_correct_list), 'std_dev': np.std(combined_correct_list), 'min': int(np.min(combined_correct_list)), 'max': int(np.max(combined_correct_list))}
    
    samples_grouped_by_count = defaultdict(list)
    for res in sample_results:
        identifier = f"{res.get('id', 'N/A')} (line {res.get('line_number', 'N/A')})"
        samples_grouped_by_count[res.get('correct_attempts', 0)].append(identifier)
            
    metrics = {
        'total_samples': total_samples,
        'pass_k_rates_combined': {f'pass_at_{k}_rate': rate for k, rate in pass_rates_combined.items()},
        'pass_k_counts_combined': {f'pass_at_{k}_count': count for k, count in pass_counts_combined.items()},
        'pass_k_rates_exact_only': {f'pass_at_{k}_rate': rate for k, rate in pass_rates_exact.items()},
        'pass_k_counts_exact_only': {f'pass_at_{k}_count': count for k, count in pass_counts_exact.items()},
        'descriptive_statistics_of_correct_attempts': descriptive_stats,
        'samples_grouped_by_correct_count': {str(k): v for k, v in sorted(samples_grouped_by_count.items(), key=lambda item: item[0], reverse=True)}
    }

    report = "\n" + "="*70 + "\n"
    report += "Pass@k è¯„ä¼°æŠ¥å‘Š (åŸºäºGPT-4o)\n"
    report += "="*70 + "\n\n"
    report += f"è¯„æµ‹é…ç½®:\n  - æ€»æ ·æœ¬æ•°: {total_samples}\n  - æ¯ä¸ªæ ·æœ¬çš„å°è¯•æ¬¡æ•°: 32 (å‡è®¾)\n\n"
    
    report += "Pass@k æ ¸å¿ƒæŒ‡æ ‡ (æ­£ç¡® = ç²¾ç¡®åŒ¹é… æˆ– éƒ¨åˆ†åŒ¹é…):\n"
    for k in pass_at_k_thresholds:
        report += f"  - Pass@{k} é€šè¿‡ç‡: {pass_rates_combined[k]:.2f}% ({pass_counts_combined[k]}/{total_samples} ä¸ªæ ·æœ¬é€šè¿‡)\n"

    report += "\næ–°å¢: Pass@k æ ¸å¿ƒæŒ‡æ ‡ (æ­£ç¡® = ä»…ç²¾ç¡®åŒ¹é…):\n"
    for k in pass_at_k_thresholds:
        report += f"  - Pass@{k} é€šè¿‡ç‡ (ç²¾ç¡®): {pass_rates_exact[k]:.2f}% ({pass_counts_exact[k]}/{total_samples} ä¸ªæ ·æœ¬é€šè¿‡)\n"
    
    report += "\næˆåŠŸå°è¯•æ¬¡æ•°çš„æè¿°æ€§ç»Ÿè®¡ (åŸºäºç»„åˆæ­£ç¡®ç‡):\n"
    report += f"  - å¹³å‡æˆåŠŸæ¬¡æ•°: {descriptive_stats['mean']:.2f}\n"
    report += f"  - æˆåŠŸæ¬¡æ•°ä¸­ä½æ•°: {descriptive_stats['median']}\n"
    report += f"  - æ ‡å‡†å·®: {descriptive_stats['std_dev']:.2f}\n"
    report += f"  - æœ€å°‘æˆåŠŸæ¬¡æ•°: {descriptive_stats['min']}\n"
    report += f"  - æœ€å¤šæˆåŠŸæ¬¡æ•°: {descriptive_stats['max']}\n"
    
    report += "\n" + "-"*40 + "\n"
    report += "æŒ‰æˆåŠŸæ¬¡æ•°åˆ†ç»„çš„é—®é¢˜åˆ—è¡¨ (åŸºäºç»„åˆæ­£ç¡®ç‡):\n"
    report += "(æ ¼å¼: <æˆåŠŸæ¬¡æ•°>: <ID (line è¡Œå·)> ...)\n"
    report += "-"*40 + "\n"
    for count, identifiers in sorted(samples_grouped_by_count.items(), key=lambda item: item[0], reverse=True):
        report += f"  - {count}: {' '.join(identifiers)}\n"
    
    report += "="*70
    print(report)
    return metrics


def main():
    args = get_args()
    
    output_dir = os.path.dirname(args.output_file)
    os.makedirs(output_dir, exist_ok=True)
    base_name = os.path.basename(args.output_file).replace('.json', '')
    intermediate_file_path = os.path.join(output_dir, f"{base_name}_intermediate.jsonl")

    all_ground_truths = read_jsonl(args.ground_truth_file)
    all_predictions = read_jsonl(args.prediction_file)

    existing_results = []
    if args.resume:
        print(f"ğŸ”„ æ¢å¤æ¨¡å¼å·²å¯åŠ¨ï¼Œæ­£åœ¨è¯»å–: {intermediate_file_path}")
        existing_results = read_jsonl(intermediate_file_path)
        if existing_results:
            processed_ids = {res['id'] for res in existing_results}
            print(f"    å·²æ‰¾åˆ° {len(processed_ids)} ä¸ªå·²å¤„ç†çš„æ ·æœ¬ã€‚")
            unprocessed_pairs = [(p, g) for p, g in zip(all_predictions, all_ground_truths) if p.get('id') not in processed_ids]
            if not unprocessed_pairs:
                predictions_to_run, ground_truths_to_run = [], []
                print("âœ… æ‰€æœ‰æ ·æœ¬å‡å·²å¤„ç†å®Œæ¯•ã€‚")
            else:
                predictions_to_run, ground_truths_to_run = zip(*unprocessed_pairs)
        else:
            print("    æœªæ‰¾åˆ°ä¸­é—´æ–‡ä»¶æˆ–æ–‡ä»¶ä¸ºç©ºï¼Œå°†ä»å¤´å¼€å§‹å¤„ç†ã€‚")
            predictions_to_run, ground_truths_to_run = all_predictions, all_ground_truths
    else:
        if os.path.exists(intermediate_file_path):
            print(f"ğŸ§¹ æ¸…ç†æ—§çš„ä¸­é—´æ–‡ä»¶: {intermediate_file_path}")
            os.remove(intermediate_file_path)
        predictions_to_run, ground_truths_to_run = all_predictions, all_ground_truths

    if predictions_to_run:
        evaluator = RobustPassKEvaluator(
            api_key=args.api_key,
            model_name=args.model_name,
            base_url=args.base_url,
            parallel_size=args.parallel_size
        )
        print(f"\nğŸš€ å¼€å§‹ä½¿ç”¨ {args.model_name} (at {args.base_url}) å¹¶è¡Œè¯„ä¼° Pass@k...")
        print(f"  - å¾…å¤„ç†æ ·æœ¬æ•°: {len(predictions_to_run)}")

        new_results = evaluator.batch_evaluate(list(predictions_to_run), list(ground_truths_to_run), intermediate_file_path)
        all_results = existing_results + new_results
    else:
        all_results = existing_results

    final_metrics = calculate_and_report_metrics(all_results)
    
    output_data = {
        'metrics': final_metrics,
        'detailed_evaluations_by_sample': sorted(all_results, key=lambda x: x.get('line_number', 0)),
        'evaluation_summary': {
            'model_used_for_evaluation': args.model_name,
            'parallel_size': args.parallel_size,
            'total_samples_evaluated': len(all_results)
        }
    }
    
    with open(args.output_file, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ‰ è¯¦ç»†è¯„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {args.output_file}")

if __name__ == "__main__":
    main()
